{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a0cb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba10edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff9b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import models\n",
    "from process_documents.prepare_professors_info import prepare_professors_info\n",
    "from process_documents.prepare_labs_info import prepare_labs_info\n",
    "professors_info_filepath = \"crawl_NU/professors_info.json\"\n",
    "labs_info_filepath = \"crawl_NU/labs_info.json\"\n",
    "\n",
    "professors_docs = prepare_professors_info(professors_info_filepath)\n",
    "labs_docs = prepare_labs_info(labs_info_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26715e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "prof_splits = prof_text_splitter.split_documents(professors_docs)\n",
    "\n",
    "lab_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 300,   \n",
    "    chunk_overlap = 30  \n",
    ")\n",
    "lab_splits = lab_text_splitter.split_documents(labs_docs)\n",
    "\n",
    "all_documents = prof_splits + lab_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6221c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "# vector_db = QdrantVectorStore.from_documents(\n",
    "#     documents,\n",
    "#     embeddings,\n",
    "#     path=\"./qdrant_db\",  \n",
    "#     collection_name=\"nu_professors\",\n",
    "# )\n",
    "\n",
    "vector_db = QdrantVectorStore.from_documents(\n",
    "    all_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\", \n",
    "    collection_name=\"nu_research\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab720d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define retriever and llm\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45cfaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query structuring for metadata filters\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "class ProfandLabSearch(BaseModel):\n",
    "    \"\"\"Search over a database of professors and labs information.\"\"\"\n",
    "\n",
    "    query: str = Field(..., description=\"The optimized search query for semantic similarity search. Focus on research topics or technical terms.\")\n",
    "\n",
    "    source_type: Optional[Literal[\"professor\", \"lab\"]] = Field(\n",
    "        None, \n",
    "        description=\"Filter results by source type. Use 'professor' for faculty and 'lab' for groups. Leave None for general queries.\"\n",
    "    )\n",
    "    professor_name: Optional[str] = Field(None, description=\"The specific name of a professor.\")\n",
    "    position: Optional[str] = Field(None, description=\"The academic position (e.g., Assistant Professor, Associate Professor).\")\n",
    "\n",
    "    lab_name: Optional[str] = Field(None, description=\"The formal name of the research laboratory.\")\n",
    "    lab_leader: Optional[str] = Field(None, description=\"The name of the faculty member heading the lab.\")\n",
    "\n",
    "    department: Optional[str] = Field(None, description=\"The department name (e.g., Computer Science, Electrical Engineering).\")\n",
    "    research_area: Optional[str] = Field(None, description=\"Broad research category (e.g., AI, Robotics, HCI).\")\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        data = self.dict()\n",
    "        for field, value in data.items():\n",
    "            if value is not None and value != \"\":\n",
    "                print(f\"{field}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7233f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of Northwestern University professors and labs information. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm = llm.with_structured_output(ProfandLabSearch)\n",
    "query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e514d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query translation\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_queries = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generated_queries = prompt_queries | llm | StrOutputParser() | (lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd698a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# retrieval_chain = generated_queries | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d10670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_logic(structured_obj):\n",
    "    query = structured_obj.query\n",
    "    queries = generated_queries.invoke({\"question\": query})\n",
    "    q_filter = None\n",
    "    filters = []\n",
    "    if structured_obj.source_type:\n",
    "        filters.append(models.FieldCondition(key=\"source_type\", match=models.MatchValue(value = structured_obj.source_type)))\n",
    "    if structured_obj.professor_name: \n",
    "        filters.append(models.FieldCondition(key=\"name\", match=models.MatchValue(value = structured_obj.professor_name)))\n",
    "    if structured_obj.position: \n",
    "        filters.append(models.FieldCondition(key=\"position\", match=models.MatchValue(value = structured_obj.position)))\n",
    "    if structured_obj.lab_name: \n",
    "        filters.append(models.FieldCondition(key=\"name\", match=models.MatchValue(value = structured_obj.lab_name)))\n",
    "    if structured_obj.lab_leader: \n",
    "        filters.append(models.FieldCondition(key=\"leader\", match=models.MatchValue(value = structured_obj.lab_leader)))\n",
    "    if structured_obj.department: \n",
    "        filters.append(models.FieldCondition(key=\"department\", match=models.MatchValue(value = structured_obj.department)))\n",
    "    if filters:\n",
    "        q_filter = models.Filter(must=filters)\n",
    "    all_docs = []\n",
    "    for q in queries:\n",
    "        docs = retriever.invoke(q, config={\"configurable\": {\"search_kwargs\": {\"filter\": q_filter}}})\n",
    "        all_docs.append(docs)\n",
    "    return get_unique_union(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de55564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = query_analyzer | RunnableLambda(query_logic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba9c5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Name: {d.metadata.get('name')}\\n\"\n",
    "        f\"Dept: {d.metadata.get('department')}\\n\"\n",
    "        f\"Contact: {d.metadata.get('contact')}\\n\"\n",
    "        f\"Position: {d.metadata.get('position', '')}\\n\"\n",
    "        f\"Website: {d.metadata.get('website', '')}\\n\"\n",
    "        f\"Related Content: {d.page_content}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant prompt based on their intent.\"\"\"\n",
    "    \n",
    "    target: Literal[\"academic_search\", \"general_research\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Choose the destination based on the user's intent:\n",
    "        - 'academic_search': Select this if the query relates to Northwestern University (NU) specifically. \n",
    "          This includes searching for specific professors, identifying research labs, inquiring about \n",
    "          departmental faculty, or finding which groups at NU work on a particular technology.\n",
    "        - 'general_research': Select this if the query is a general scientific or technical question \n",
    "          that does NOT require Northwestern-specific data. This includes explaining terminologies \n",
    "          (e.g., 'What is Cross-Entropy?'), helping with general research methodology, or \n",
    "          conceptual explanations that apply universally regardless of the institution.\n",
    "        \"\"\"\n",
    "    )\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\"You are an expert at routing a user's question to the most relevant pipeline.\n",
    "1. 'academic_search': For questions about Northwestern University's McCormick School of Engineering, \n",
    "   including specific professors, labs, departments, or finding local experts in a field.\n",
    "2. 'general_research': For general scientific knowledge, terminology explanations, or \n",
    "   research concepts that are independent of any specific institution.\n",
    "\n",
    "If the user mentions a name or a lab that sounds like it belongs to an institution, default to 'academic_search'.\"\"\"\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "system_content_choice_1 = \"\"\"You are an expert academic research assistant specializing in the Northwestern University McCormick School of Engineering faculty and research laboratories.\n",
    "\n",
    "Your goal is to provide comprehensive information about professors and their associated labs based on the provided context.\n",
    "\n",
    "### Guidelines:\n",
    "1. **Entity Linking**: Always identify the connection between professors and labs. If the context mentions a professor leads a specific lab (e.g., 'AquaLab'), ensure both are mentioned together.\n",
    "2. **Contextual Retrieval**:\n",
    "    - **For Professor Queries**: Provide their name, department, position, and the primary research areas they focus on.\n",
    "    - **For Lab Queries**: Provide the lab name, the faculty director (Professor), and the specific projects or technologies the lab is developing.\n",
    "3. **Technical Mapping**: If a user's technical term (e.g., 'LLM') doesn't have a literal match, bridge the gap by referring to broader context terms like 'Natural Language Processing', 'Machine Learning', or 'Artificial Intelligence'.\n",
    "4. **Structured Response**:\n",
    "    - Mention the **Department** and **Contact/Website** info if available in the context.\n",
    "    - If multiple professors/labs are relevant, list them clearly with brief descriptions of their distinct focus.\n",
    "5. **Strict Grounding & Integrity**: \n",
    "    - Only answer based on the **Context** provided. \n",
    "    - If the context lacks a specific professor or lab for a topic, state: \"Based on the current Northwestern database, I couldn't find a specific professor or lab researching [Topic].\" \n",
    "    - Do not hallucinate names or affiliations not present in the context.\n",
    "\"\"\"\n",
    "\n",
    "system_content_choice_2 = \"\"\" You are a professional research scientist. \n",
    "Your expertise is in general concepts. If the user mentions specific universities, professors, or faculty, do not try to search for local data; instead, explain the scientific concepts behind their inquiry\n",
    "You are great at answering general research-related question, explain terminologies and concepts, and help plan experiments\n",
    "You will answer all questions in a concise and easy to understand manner, explain in detail if the user asks.\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\"\"\"\n",
    "\n",
    "def choose_prompt(result):\n",
    "    if \"academic_search\" in result.target.lower():\n",
    "        return system_content_choice_1\n",
    "    else:\n",
    "        return system_content_choice_2\n",
    "\n",
    "selected_prompt = router_prompt | structured_llm | RunnableLambda(choose_prompt) \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system_message}\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"system_message\": selected_prompt, \"context\": retrieval_chain | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "983a79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain_core._api import LangChainBetaWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=LangChainBetaWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c29f39d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several labs focus on research related to Large Language Models (LLMs) and their applications. Here are a few notable ones based on the context provided:\n",
      "\n",
      "1. **Machine Learning and Language Lab**: This lab develops intelligent language models that integrate with various domains, such as vision and robotics. Their work likely involves LLMs as they explore how these models can reason, plan, and interact with the physical world.\n",
      "\n",
      "2. **Language and Computation Lab** (led by Klinton Bicknell): This lab investigates how the human brain processes language and may utilize LLMs to build computational models of language behaviors. Their research could involve analyzing how LLMs can mimic or enhance human language processing.\n",
      "\n",
      "3. **Research by Yiping Lu**: While not a lab per se, Yiping Lu's research on scaling laws in machine learning is highly relevant to LLMs. His work focuses on understanding how performance improves as resources are scaled, which is a critical aspect of developing and optimizing LLMs.\n",
      "\n",
      "These labs and research areas contribute to the understanding and advancement of Large Language Models in various contexts, including natural language processing, machine learning, and human-computer interaction."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream({\"question\": \"What labs are related to Large Language Models?\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
