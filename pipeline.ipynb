{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a0cb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba10edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langsmith import Client\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2365138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "import json\n",
    "\n",
    "with open('output.json', 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# reformatting the keys and make up content to store\n",
    "documents = []\n",
    "for item in raw_data:\n",
    "    cleaned_item = {}\n",
    "    for key, value in item.items():\n",
    "        if not value: continue\n",
    "        k = key.lower().strip()\n",
    "\n",
    "        if \"research\" in k:\n",
    "            target_key = \"research\"\n",
    "        elif any(x in k for x in [\"publications\", \"books\", \"project\", \"patents\"]):\n",
    "            target_key = \"publications\"\n",
    "        elif any(x in k for x in [\"courses\", \"lectures\"]):\n",
    "            target_key = \"courses\"\n",
    "        elif any(x in k for x in [\"teaching\", \"students\", \"classroom\"]):\n",
    "            target_key = \"teaching\"\n",
    "        elif \"experience\" in k:\n",
    "            target_key = \"experience\"\n",
    "        elif \"website\" in k:\n",
    "            target_key = \"website\"\n",
    "        elif \"news\" in k:\n",
    "            target_key = \"news\"\n",
    "        elif k in [\"positions\", \"position\"]:\n",
    "            target_key = \"position\"\n",
    "        else:\n",
    "            target_key = k\n",
    "        if target_key in cleaned_item:\n",
    "            if value not in cleaned_item[target_key]:\n",
    "                cleaned_item[target_key] += \"\\n\" + value\n",
    "        else:\n",
    "            cleaned_item[target_key] = value\n",
    "\n",
    "    search_parts = [\n",
    "        f\"Research & Expertise: {cleaned_item.get('research', '')}\",\n",
    "        f\"Publications & Works: {cleaned_item.get('publications', '')}\",\n",
    "        f\"Biography: {cleaned_item.get('biography', '')}\",\n",
    "        f\"Experience: {cleaned_item.get('experience', '')}\",\n",
    "        f\"Courses & Teaching: {cleaned_item.get('courses', '')} {cleaned_item.get('teaching', '')}\"\n",
    "    ]\n",
    "\n",
    "    full_search_text = \"\\n\".join([p for p in search_parts if len(p) > 25])\n",
    "\n",
    "    metadata = {\n",
    "        \"name\": cleaned_item.get(\"name\"),\n",
    "        \"position\": cleaned_item.get(\"position\"),\n",
    "        \"department\": cleaned_item.get(\"departments\"),\n",
    "        \"contact\": cleaned_item.get(\"contact\"),\n",
    "        \"website\": cleaned_item.get(\"website\")\n",
    "    }\n",
    "    if full_search_text.strip():\n",
    "        doc = Document(page_content=full_search_text, metadata=metadata)\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26715e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6221c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "# vector_db = QdrantVectorStore.from_documents(\n",
    "#     documents,\n",
    "#     embeddings,\n",
    "#     path=\"./qdrant_db\",  \n",
    "#     collection_name=\"nu_professors\",\n",
    "# )\n",
    "vector_db = QdrantVectorStore.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\", \n",
    "    collection_name=\"nu_professors\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62531d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free version\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# vector_db = QdrantVectorStore.from_documents(\n",
    "#     documents,\n",
    "#     embeddings,\n",
    "#     location=\":memory:\", \n",
    "#     collection_name=\"nu_professors\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab720d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define retriever and llm\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e514d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query translation\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_queries = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generated_queries = prompt_queries | llm | StrOutputParser() | (lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "776d6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "retrieval_chain = generated_queries | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9c5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Name: {d.metadata.get('name')}\\n\"\n",
    "        f\"Dept: {d.metadata.get('department')}\\n\"\n",
    "        f\"Contact: {d.metadata.get('contact')}\\n\"\n",
    "        f\"Position: {d.metadata.get('position', '')}\\n\"\n",
    "        f\"Website: {d.metadata.get('website', '')}\\n\"\n",
    "        f\"Related Content: {d.page_content}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant prompt based on their intent.\"\"\"\n",
    "    target: Literal[\"academic_search\", \"general_research\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Choose the destination for the user's query:\n",
    "        - 'academic_search': Select this if the user is asking about specific professors at Northwestern University, their labs, departments, or seeking specific faculty members.\n",
    "        - 'general_research': Select this if the user is asking general scientific questions, explaining terminologies (like what is LLM), or needs help with research concepts without referring to a specific person.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\" You are an expert at routing a user's question to the most relevant prompt based on their intent. \n",
    "Choose the prompt based on whether the user is asking about specific professors and their research in Northwestern University or general questions\n",
    "\"\"\"\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "system_content_choice_1 = \"\"\"You are an expert academic research assistant specializing in the faculty of Northwestern University's McCormick School of Engineering.\n",
    "\n",
    "Your goal is to help users find the most relevant professors based on the provided context. \n",
    "\n",
    "Guidelines:\n",
    "1. **Identify Professors**: The context contains faculty names, departments, and research interests. Always mention the professor's name clearly.\n",
    "2. **Handle Technical Terms**: If a user asks about a specific term like 'RAG', search the context for related fields like 'Natural Language Processing', 'Information Retrieval', or 'Artificial Intelligence' if a direct match isn't found.\n",
    "3. **Be Specific**: Mention the department or specific lab if available.\n",
    "4. **Admit Ignorance**: If the context does not contain information about a professor doing that specific research, explicitly state: \"Based on the current database, no professor was found specifically researching [Topic].\" Do not hallucinate or list irrelevant professors.\n",
    "\"\"\"\n",
    "\n",
    "system_content_choice_2 = \"\"\" You are a professional research scientist. \n",
    "Your expertise is in general concepts. If the user mentions specific universities, professors, or faculty, do not try to search for local data; instead, explain the scientific concepts behind their inquiry\n",
    "You are great at answering general research-related question, explain terminologies and concepts, and help plan experiments\n",
    "You will answer all questions in a concise and easy to understand manner, explain in detail if the user asks.\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\"\"\"\n",
    "\n",
    "def choose_prompt(result):\n",
    "    if \"academic_search\" in result.target.lower():\n",
    "        return system_content_choice_1\n",
    "    else:\n",
    "        return system_content_choice_2\n",
    "\n",
    "selected_prompt = router_prompt | structured_llm | RunnableLambda(choose_prompt) \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system_message}\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"system_message\": selected_prompt, \"context\": retrieval_chain | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983a79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain_core._api import LangChainBetaWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=LangChainBetaWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f39d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the current database, no professor was found specifically researching under the name \"Lizhen Shi.\" However, there is a Professor Naichen Shi, who leads the Integrative Artificial Intelligence Lab, focusing on advancing integrative and generative AI methods for aligning heterogeneous data and knowledge across complex engineering systems, particularly in advanced manufacturing. If you meant Naichen Shi, please let me know!"
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream({\"question\": \"Who is doing research on CS + Biology?\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba4d5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG, or Retrieval-Augmented Generation, is a technique in natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models. The primary goal of RAG is to enhance the quality and relevance of generated text by incorporating external knowledge from a retrieval system.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. **Retrieval Component**: When a query or prompt is given, the system first retrieves relevant documents or pieces of information from a large corpus or database. This retrieval is typically done using techniques like vector similarity search or traditional keyword matching.\n",
      "\n",
      "2. **Generation Component**: After retrieving the relevant information, a generative model (often based on transformer architectures like GPT or BERT) uses this information to produce a coherent and contextually relevant response. The generative model can leverage the retrieved content to provide more accurate, informative, and contextually appropriate answers.\n",
      "\n",
      "3. **Integration**: The integration of the retrieval and generation processes allows the model to produce responses that are not only fluent and natural but also grounded in factual information. This is particularly useful in applications like question answering, chatbots, and summarization, where accuracy and relevance are crucial.\n",
      "\n",
      "RAG models can significantly improve performance on tasks that require up-to-date knowledge or specific information that may not be contained within the model's training data. By combining retrieval with generation, RAG effectively addresses some limitations of purely generative models, such as the risk of generating incorrect or nonsensical information.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is RAG in NLP?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
