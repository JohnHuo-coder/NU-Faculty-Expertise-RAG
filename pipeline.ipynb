{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a0cb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from process_documents.prepare_professors_info import prepare_professors_info\n",
    "from process_documents.prepare_labs_info import prepare_labs_info\n",
    "professors_info_filepath = \"crawlNU/professors_info.json\"\n",
    "labs_info_filepath = \"crawl_NU/labs_info.json\"\n",
    "\n",
    "professors_docs = prepare_professors_info(professors_info_filepath)\n",
    "labs_docs = prepare_labs_info(labs_info_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26715e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "splits = text_splitter.split_documents(professors_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6221c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "# vector_db = QdrantVectorStore.from_documents(\n",
    "#     documents,\n",
    "#     embeddings,\n",
    "#     path=\"./qdrant_db\",  \n",
    "#     collection_name=\"nu_professors\",\n",
    "# )\n",
    "vector_db = QdrantVectorStore.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\", \n",
    "    collection_name=\"nu_professors\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62531d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free version\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# vector_db = QdrantVectorStore.from_documents(\n",
    "#     documents,\n",
    "#     embeddings,\n",
    "#     location=\":memory:\", \n",
    "#     collection_name=\"nu_professors\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab720d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define retriever and llm\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e514d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query translation\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_queries = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generated_queries = prompt_queries | llm | StrOutputParser() | (lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "776d6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "retrieval_chain = generated_queries | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9c5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Name: {d.metadata.get('name')}\\n\"\n",
    "        f\"Dept: {d.metadata.get('department')}\\n\"\n",
    "        f\"Contact: {d.metadata.get('contact')}\\n\"\n",
    "        f\"Position: {d.metadata.get('position', '')}\\n\"\n",
    "        f\"Website: {d.metadata.get('website', '')}\\n\"\n",
    "        f\"Related Content: {d.page_content}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant prompt based on their intent.\"\"\"\n",
    "    target: Literal[\"academic_search\", \"general_research\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Choose the destination for the user's query:\n",
    "        - 'academic_search': Select this if the user is asking about specific professors at Northwestern University, their labs, departments, or seeking specific faculty members.\n",
    "        - 'general_research': Select this if the user is asking general scientific questions, explaining terminologies (like what is LLM), or needs help with research concepts without referring to a specific person.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\" You are an expert at routing a user's question to the most relevant prompt based on their intent. \n",
    "Choose the prompt based on whether the user is asking about specific professors and their research in Northwestern University or general questions\n",
    "\"\"\"\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "system_content_choice_1 = \"\"\"You are an expert academic research assistant specializing in the faculty of Northwestern University's McCormick School of Engineering.\n",
    "\n",
    "Your goal is to help users find the most relevant professors based on the provided context. \n",
    "\n",
    "Guidelines:\n",
    "1. **Identify Professors**: The context contains faculty names, departments, and research interests. Always mention the professor's name clearly.\n",
    "2. **Handle Technical Terms**: If a user asks about a specific term like 'RAG', search the context for related fields like 'Natural Language Processing', 'Information Retrieval', or 'Artificial Intelligence' if a direct match isn't found.\n",
    "3. **Be Specific**: Mention the department or specific lab if available.\n",
    "4. **Admit Ignorance**: If the context does not contain information about a professor doing that specific research, explicitly state: \"Based on the current database, no professor was found specifically researching [Topic].\" Do not hallucinate or list irrelevant professors.\n",
    "\"\"\"\n",
    "\n",
    "system_content_choice_2 = \"\"\" You are a professional research scientist. \n",
    "Your expertise is in general concepts. If the user mentions specific universities, professors, or faculty, do not try to search for local data; instead, explain the scientific concepts behind their inquiry\n",
    "You are great at answering general research-related question, explain terminologies and concepts, and help plan experiments\n",
    "You will answer all questions in a concise and easy to understand manner, explain in detail if the user asks.\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\"\"\"\n",
    "\n",
    "def choose_prompt(result):\n",
    "    if \"academic_search\" in result.target.lower():\n",
    "        return system_content_choice_1\n",
    "    else:\n",
    "        return system_content_choice_2\n",
    "\n",
    "selected_prompt = router_prompt | structured_llm | RunnableLambda(choose_prompt) \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system_message}\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"system_message\": selected_prompt, \"context\": retrieval_chain | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983a79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain_core._api import LangChainBetaWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=LangChainBetaWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c29f39d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the current database, the following professors are involved in research related to Large Language Models:\n",
      "\n",
      "1. **Yiping Lu** - Assistant Professor of Industrial Engineering and Management Sciences. His research focuses on scaling laws in machine learning, which includes large language models and their performance as resources are scaled.\n",
      "\n",
      "2. **Manling Li** - Assistant Professor of Computer Science. Her research includes topics related to large language models, specifically in the context of action models and their applications.\n",
      "\n",
      "3. **Larry Birnbaum** - Professor of Computer Science. His research includes natural language processing (NLP) and the application of AI in generating narratives, which is relevant to large language models.\n",
      "\n",
      "If you need more specific information about their research or publications, feel free to ask!"
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream({\"question\": \"Any professor doing research on Large Language Model?\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba4d5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG, or Retrieval-Augmented Generation, is a technique in natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models. The primary goal of RAG is to enhance the quality and relevance of generated text by incorporating external knowledge from a retrieval system.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. **Retrieval Component**: When a query or prompt is given, the system first retrieves relevant documents or pieces of information from a large corpus or database. This retrieval is typically done using techniques like vector similarity search or traditional keyword matching.\n",
      "\n",
      "2. **Generation Component**: After retrieving the relevant information, a generative model (often based on transformer architectures like GPT or BERT) uses this information to produce a coherent and contextually relevant response. The generative model can leverage the retrieved content to provide more accurate, informative, and contextually appropriate answers.\n",
      "\n",
      "3. **Integration**: The integration of the retrieval and generation processes allows the model to produce responses that are not only fluent and natural but also grounded in factual information. This is particularly useful in applications like question answering, chatbots, and summarization, where accuracy and relevance are crucial.\n",
      "\n",
      "RAG models can significantly improve performance on tasks that require up-to-date knowledge or specific information that may not be contained within the model's training data. By combining retrieval with generation, RAG effectively addresses some limitations of purely generative models, such as the risk of generating incorrect or nonsensical information.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is RAG in NLP?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
